

import math
import os
import logging

import config
os.environ['TRANSFORMERS_CACHE'] = config.TRANSFORMERS_PATH

import torch
import numpy as np
import random

from transformers import GPT2LMHeadModel, AutoConfig

from training import *
from dataloading import *
from models import *
from evaluations import *

import argparse

if torch.cuda.is_available():
    device = torch.device('cuda')
else:
    device = torch.device('cpu')


parser = argparse.ArgumentParser()

# Data arguments
parser.add_argument("--directory", help="Directory where the datasets are found", type=str, default="CHILDES_final/pretraining/")
parser.add_argument("--add_eos", help="Add EOS at the end of each line", action='store_true')
parser.add_argument("--batch_size", help="Sequences per batch", type=int, default=128)
parser.add_argument("--shuffle", help="Shuffle batches within buffer", action='store_true')

# Architecture arguments
parser.add_argument("--architecture", help="Type of architecture. Options: GPT2, LSTM", type=str, default="GPT2")
parser.add_argument("--n_embd", help="Embedding size; also used as the hidden size", type=int, default=768)
parser.add_argument("--n_positions", help="Max context length the model can take", type=int, default=128)
parser.add_argument("--n_head", help="Number of attention heads", type=int, default=12)
parser.add_argument("--n_layer", help="Number of layers", type=int, default=12)
parser.add_argument("--dropout", help="Dropout", type=float, default=0.1)
parser.add_argument("--pretrained_name", help="Name for a pretrained model to load", type=str, default=None)
parser.add_argument("--pretrained_vocab_size", help="Vocab size in pretrained model", type=int, default=None)

# Training arguments
parser.add_argument("--n_epochs", help="Number of training epochs", type=int, default=10)
parser.add_argument("--eval_every", help="Number of training steps to go between evaluations", type=int, default=100)
parser.add_argument("--weight_decay", help="Weight decay", type=float, default=1e-1)
parser.add_argument("--learning_rate", help="Learning rate", type=float, default=5e-4)
parser.add_argument("--lr_scheduler_type", help="Learning rate scheduler type (cosine or constant)", type=str, default="cosine")
parser.add_argument("--warmup_proportion", help="Proportion of total steps that are warmup", type=float, default=0.05)
parser.add_argument("--patience", help="Patience", type=int, default=None)
parser.add_argument("--lr_decay_patience", help="Learning rate decay paatience", type=int, default=None)

# Saving arguments
parser.add_argument("--model_name", help="Model name prefix", type=str, default=None)
parser.add_argument("--model_index", help="Model name index", type=int, default=None)
parser.add_argument("--weight_dir", help="Directory to save model weights in", type=str, default=config.WEIGHT_DIR)
parser.add_argument("--log_dir", help="Directory to save logs in", type=str, default=config.LOG_DIR)

# Evaluation arguments
parser.add_argument("--eval", help="Just evaluating, no training", action='store_true')
parser.add_argument("--eval_generate", help="Print some examples of text generated by the trained model", action='store_true')
parser.add_argument("--eval_zorro", help="Evaluate on minimal pairs from the Zorro dataset", action='store_true')
parser.add_argument("--eval_recursion", help="Evaluate on minimal pairs from the recursion dataset", action='store_true')
parser.add_argument("--eval_priming", help="Evaluate on priming perplexity", action='store_true')
parser.add_argument("--eval_scamp_plausible", help="Evaluate on scamp-plausible", action='store_true')
parser.add_argument("--eval_scamp_implausible", help="Evaluate on scamp-implausible", action='store_true')
parser.add_argument("--eval_blimp", help="Evaluate on minimal pairs from the Blimp dataset", action='store_true')
parser.add_argument("--eval_test", help="Evaluate on the test set", action='store_true')
parser.add_argument("--eval_suffix", help="Suffix to add to the evaluation log file", type=str, default=None)
parser.add_argument("--eval_save_items", help="Save all item-level results for the evaluation", action='store_true')
args = parser.parse_args()



################################################################################################
# Preliminaries
################################################################################################

if not args.eval:
    if args.model_index is not None:
        model_name = args.model_name
        model_index = args.model_index
        args.model_name = model_name + "_" + str(model_index)
        if args.model_name + ".log" in os.listdir(args.log_dir):
            raise Exception("Model index is already in use")

    else:
        model_name = args.model_name
        model_index = 0
        args.model_name = model_name + "_" + str(model_index)
        while args.model_name + ".log" in os.listdir(args.log_dir):
            model_index += 1
            args.model_name = model_name + "_" + str(model_index)

    random.seed(model_index)
    np.random.seed(model_index)
    torch.manual_seed(model_index)



if args.eval:
    if args.eval_suffix is not None:
        logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', level=logging.INFO, handlers=[logging.StreamHandler(),logging.FileHandler(args.log_dir + args.model_name + "_eval_" + args.eval_suffix + ".log")])
    else:
        logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', level=logging.INFO, handlers=[logging.StreamHandler(),logging.FileHandler(args.log_dir + args.model_name + "_eval.log")])

else:
    logging.basicConfig(format='%(asctime)s %(levelname)-8s %(message)s', level=logging.INFO, handlers=[logging.StreamHandler(),logging.FileHandler(args.log_dir + args.model_name + ".log")])

logging.info(args)


dataset = LMDatasetFromFiles(directory=args.directory, add_eos=args.add_eos, batch_size=args.batch_size, context_size=args.n_positions, batches_per_buffer=10, loop=False, stream=True, shuffle=args.shuffle)

logging.info("Vocab size: " + str(len(dataset.tokenizer)+1))

if args.pretrained_vocab_size is not None:
    vocab_size = args.pretrained_vocab_size
else:
    vocab_size = len(dataset.tokenizer)+1



################################################################################################
# Create the model
################################################################################################

if args.architecture == "GPT2":

    model = GPT2LM(vocab_size=vocab_size, emb_size=args.n_embd, n_positions=args.n_positions, n_layer=args.n_layer, n_head=args.n_head, dropout=args.dropout, save_dir=args.weight_dir, model_name=args.model_name).to(device)

elif args.architecture == "LSTM":

    model = RNNLM(rnn_type="LSTM", vocab_size=vocab_size, emb_size=args.n_embd, hidden_size=args.n_embd, n_layers=args.n_layer, dropout=args.dropout, tie_weights=True, save_dir=args.weight_dir, model_name=args.model_name).to(device)

else:
    logging.info("Architecture not recognized")

model_size = sum(t.numel() for t in model.parameters())
logging.info(f"Model size: {model_size/1000**2:.1f}M parameters")




################################################################################################
# Create the trainer and train
################################################################################################

args.warmup_steps = math.ceil(args.warmup_proportion*args.n_epochs*len(dataset.train))

trainer = Trainer(
            model=model,
            train_datasplit=dataset.train,
            eval_datasplit=dataset.valid,
            n_epochs=args.n_epochs,
            patience=args.patience,
            lr_decay_patience=args.lr_decay_patience,
            weight_decay=args.weight_decay,
            learning_rate=args.learning_rate,
            lr_scheduler_type=args.lr_scheduler_type,
            warmup_steps=args.warmup_steps,
            eval_every=args.eval_every,
            log_every=args.eval_every,
            tokenizer=dataset.tokenizer,
        )


if args.pretrained_name is not None:
    trainer.model.load(args.pretrained_name)
    trainer.model.emb_layer = nn.Embedding(len(dataset.tokenizer)+1, trainer.model.emb_size)
    trainer.model.out_layer = nn.Linear(trainer.model.hidden_size, len(dataset.tokenizer)+1)
    trainer.model.out_layer.weight = trainer.model.emb_layer.weight
    trainer.model.to(device=device)


if not args.eval:
    trainer.train()

# Load the saved best weights
trainer.model.load()




################################################################################################
# Evaluate
################################################################################################

if args.eval_generate:
    
    for _ in range(3):
        sample_generation(dataset, trainer, n_positions=args.n_positions)

if args.eval_save_items:
    save_name = args.model_name
else:
    save_name = None


if args.eval_zorro:
    # Evaluate on minimal pairs from the Zorro dataset
    zorro_eval(model, dataset, save_name=save_name)

if args.eval_blimp:
    # Evaluate on minimal pairs from the Blimp dataset
    blimp_eval(model, dataset, save_name=save_name)

if args.eval_scamp_plausible:
    # Evaluate on scamp-plausible
    scamp_eval(model, dataset, plausible=True, save_name=save_name)

if args.eval_scamp_implausible:
    # Evaluate on scamp-implausible
    scamp_eval(model, dataset, plausible=False, save_name=save_name)


if args.eval_recursion:
    # Evaluate on recursion dataset
    recursion_eval(model, dataset, save_name=save_name)

if args.eval_priming:
    # Evaluate on priming
    priming_eval(model, dataset, save_name=save_name)


# No matter what, evaluate on the validation set
trainer.evaluate(eval_datasplit=dataset.valid, name="Validation")
trainer.evaluate(eval_datasplit=dataset.stride_valid, name="Validation (strided)")


if args.eval_test:
    # Evaluate on the test set
    trainer.evaluate(eval_datasplit=dataset.test, name="Test")
    trainer.evaluate(eval_datasplit=dataset.stride_test, name="Test (strided)")







